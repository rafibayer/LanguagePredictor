{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\" # list of character\n",
    "MAX_WORD_LEN = 15 # longest word our network can parse\n",
    "ONE_HOT_SIZE = len(ALPHABET) * MAX_WORD_LEN # how long our one hot array should be\n",
    "\n",
    "# get position of a char in alphabet\n",
    "def getPos(char):\n",
    "    if len(char) > 1:\n",
    "        raise Exception(\"char should be a single character, length was: {}\".format(len(char)))\n",
    "    char = char.lower()\n",
    "    if char not in ALPHABET:\n",
    "        raise Exception(\"char should be in the alphabet, char was: {}\".format(char))\n",
    "    return ALPHABET.index(char)\n",
    "\n",
    "# create a one hot array for value, length of length\n",
    "def oneHot(val, length):\n",
    "    arr = [0]*length\n",
    "    arr[val] = 1\n",
    "    return arr\n",
    "\n",
    "# get one hot array for single char\n",
    "def charOneHot(char):\n",
    "    val = getPos(char)\n",
    "    return oneHot(val, len(ALPHABET))\n",
    "\n",
    "# get one hot arrays for each char in a string\n",
    "def oneHotString(string):\n",
    "    strLen = len(string)\n",
    "    result = [0]*strLen\n",
    "    for i in range(0, strLen):\n",
    "        result[i] = charOneHot(string[i])\n",
    "        \n",
    "    return result\n",
    "\n",
    "# transform a word to a one hot array \n",
    "def wordToInput(word):\n",
    "    if len(word) > MAX_WORD_LEN:\n",
    "        raise Exception(\"Word is too long, must be < 15 char, actual: {}\".format(len(word)))\n",
    "    result = [0]*ONE_HOT_SIZE\n",
    "    index = 0\n",
    "    for char in word:\n",
    "        result[getPos(char) + index] = 1\n",
    "        index = index + len(ALPHABET)\n",
    "    return result\n",
    "    \n",
    "# great, now we can turn words into one hot arrays\n",
    "# now we have to get and label training data\n",
    "\n",
    "# functions for data wrangling\n",
    "\n",
    "# returns true if word only contains letters in alphabet\n",
    "def isAlpha(word):\n",
    "    word = word.lower()\n",
    "    for char in word:\n",
    "        if char not in ALPHABET:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts before slice:\n",
      "English words: 44295\n",
      "German words: 39692\n",
      "French words: 33678 \n",
      "\n",
      "Word counts after slice:\n",
      "English words: 33000\n",
      "German words: 33000\n",
      "French words: 33000\n"
     ]
    }
   ],
   "source": [
    "LANGUAGES = [\"English\", \"German\", \"French\"]\n",
    "MIN_WORD_LEN = 3\n",
    "\n",
    "# dictionary source: https://github.com/hermitdave/FrequencyWords\n",
    "\n",
    "# data wrangling time, things we have to do:\n",
    "# remove words shorter than 3(not required but it'll help) \n",
    "# remove words containing words outside of the english alphabet (this means no accents)\n",
    "# remove words longer than 15 characters (or whatever we decide our maximum to be)\n",
    "# the dataset I'm using also has frequency for each word, we don't need that\n",
    "\n",
    "# parsing dictionaries\n",
    "english = open(\"english_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "english_lines = english.readlines()\n",
    "english_words = []\n",
    "for line in english_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        english_words.append(word)        \n",
    "        \n",
    "german = open(\"german_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "german_lines = german.readlines()\n",
    "german_words = []\n",
    "for line in german_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        german_words.append(word)\n",
    "        \n",
    "french = open(\"french_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "french_lines = french.readlines()\n",
    "french_words = []\n",
    "for line in french_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        french_words.append(word)\n",
    "        \n",
    "print(\"Word counts before slice:\")\n",
    "print(\"English words: {}\".format(len(english_words)))\n",
    "print(\"German words: {}\".format(len(german_words)))\n",
    "print(\"French words: {} \\n\".format(len(french_words)))\n",
    "\n",
    "\n",
    "#balancing\n",
    "SAMPLES = 33000 # number of words for each language, should be less than smallest dictionary\n",
    "\n",
    "english_words = english_words[0:SAMPLES]\n",
    "german_words = german_words[0:SAMPLES]\n",
    "french_words = french_words[0:SAMPLES]\n",
    "\n",
    "print(\"Word counts after slice:\")\n",
    "print(\"English words: {}\".format(len(english_words)))\n",
    "print(\"German words: {}\".format(len(german_words)))\n",
    "print(\"French words: {}\".format(len(french_words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to label data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# converting words to one hot encoding\n",
    "X_train = []\n",
    "for word in english_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "\n",
    "for word in german_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "    \n",
    "for word in french_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# labeling data\n",
    "y_train = []\n",
    "y_train.extend([0]*SAMPLES) #English\n",
    "y_train.extend([1]*SAMPLES) #German\n",
    "y_train.extend([2]*SAMPLES) #French\n",
    "\n",
    "# converting labels to one hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# shuffling data\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "# data labeled in np array\n",
    "\n",
    "\n",
    "# Validation slice\n",
    "\n",
    "NUM_VALIDATE = 5000\n",
    "\n",
    "X_test = X_train[len(X_train) - NUM_VALIDATE:len(X_train)]\n",
    "y_test = y_train[len(y_train) - NUM_VALIDATE:len(y_train)]\n",
    "\n",
    "X_train = X_train[0:len(X_train) - NUM_VALIDATE]\n",
    "y_train = y_train[0:len(y_train) - NUM_VALIDATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to load serialized data\n",
    "import pickle\n",
    "X_train = pickle.load(open(\"X.pickle\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model: L32-D0-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 6s 59us/sample - loss: 0.8493 - acc: 0.5911\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 62us/sample - loss: 0.7293 - acc: 0.6549\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 6s 64us/sample - loss: 0.6823 - acc: 0.6756\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 5s 56us/sample - loss: 0.6513 - acc: 0.6899\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 5s 54us/sample - loss: 0.6280 - acc: 0.6991\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 5s 53us/sample - loss: 0.6088 - acc: 0.7041\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 5s 53us/sample - loss: 0.5927 - acc: 0.7098\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 5s 51us/sample - loss: 0.5793 - acc: 0.7127\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 5s 52us/sample - loss: 0.5681 - acc: 0.7172\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 5s 52us/sample - loss: 0.5576 - acc: 0.7197\n",
      "evaluate model: L32-D0-H2\n",
      "5000/5000 [==============================] - 0s 58us/sample - loss: 0.8434 - acc: 0.6340\n",
      "training model: L32-D0.33-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 6s 62us/sample - loss: 0.8968 - acc: 0.5629\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 59us/sample - loss: 0.7712 - acc: 0.6401\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.7204 - acc: 0.6613\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 5s 57us/sample - loss: 0.6868 - acc: 0.6748\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 5s 57us/sample - loss: 0.6651 - acc: 0.6820\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6453 - acc: 0.6879\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 5s 57us/sample - loss: 0.6300 - acc: 0.6950\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6164 - acc: 0.6971\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6038 - acc: 0.7021\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.5953 - acc: 0.7044\n",
      "evaluate model: L32-D0.33-H2\n",
      "5000/5000 [==============================] - 0s 68us/sample - loss: 0.8074 - acc: 0.6386\n",
      "training model: L32-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 6s 63us/sample - loss: 0.9373 - acc: 0.5343\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 59us/sample - loss: 0.8098 - acc: 0.6227\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.7531 - acc: 0.6512\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 6s 60us/sample - loss: 0.7180 - acc: 0.6650\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6954 - acc: 0.6745\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 6s 60us/sample - loss: 0.6743 - acc: 0.6808\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 6s 59us/sample - loss: 0.6586 - acc: 0.6875\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 6s 60us/sample - loss: 0.6434 - acc: 0.6931\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6334 - acc: 0.6951\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.6226 - acc: 0.6976\n",
      "evaluate model: L32-D0.5-H2\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.7764 - acc: 0.6414\n",
      "training model: L64-D0-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 6s 64us/sample - loss: 0.8433 - acc: 0.5942\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 60us/sample - loss: 0.7280 - acc: 0.6556\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 6s 62us/sample - loss: 0.6807 - acc: 0.6763\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.6505 - acc: 0.6895\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6254 - acc: 0.6969\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.6055 - acc: 0.7047\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.5897 - acc: 0.7093\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.5743 - acc: 0.7130\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 6s 66us/sample - loss: 0.5619 - acc: 0.7181\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5516 - acc: 0.7205\n",
      "evaluate model: L64-D0-H2\n",
      "5000/5000 [==============================] - 0s 82us/sample - loss: 0.8371 - acc: 0.6300\n",
      "training model: L64-D0.33-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 85us/sample - loss: 0.8771 - acc: 0.5759\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.7517 - acc: 0.6463\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.7017 - acc: 0.6676\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.6716 - acc: 0.6799\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.6446 - acc: 0.6904\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6276 - acc: 0.6957\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6117 - acc: 0.7017\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 8s 86us/sample - loss: 0.5966 - acc: 0.7056\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.5859 - acc: 0.7085\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.5743 - acc: 0.7117\n",
      "evaluate model: L64-D0.33-H2\n",
      "5000/5000 [==============================] - 0s 90us/sample - loss: 0.8240 - acc: 0.6394\n",
      "training model: L64-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.9032 - acc: 0.5594\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 70us/sample - loss: 0.7747 - acc: 0.6393\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 6s 68us/sample - loss: 0.7235 - acc: 0.6601\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 6s 67us/sample - loss: 0.6898 - acc: 0.6724\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 6s 67us/sample - loss: 0.6677 - acc: 0.6825\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.6491 - acc: 0.6886\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.6291 - acc: 0.6939\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.6158 - acc: 0.6991\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.6035 - acc: 0.7029\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 6s 69us/sample - loss: 0.5926 - acc: 0.7065\n",
      "evaluate model: L64-D0.5-H2\n",
      "5000/5000 [==============================] - 0s 81us/sample - loss: 0.7854 - acc: 0.6466\n",
      "training model: L128-D0-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 7s 78us/sample - loss: 0.8392 - acc: 0.5942\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.7224 - acc: 0.6578\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6758 - acc: 0.6783\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6436 - acc: 0.6895\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6190 - acc: 0.6987\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.5984 - acc: 0.7065\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.5803 - acc: 0.7120\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5658 - acc: 0.7165\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5523 - acc: 0.7200\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.5423 - acc: 0.7224\n",
      "evaluate model: L128-D0-H2\n",
      "5000/5000 [==============================] - 0s 84us/sample - loss: 0.8838 - acc: 0.6374\n",
      "training model: L128-D0.33-H2\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94000/94000 [==============================] - 8s 90us/sample - loss: 0.8624 - acc: 0.5830\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 8s 85us/sample - loss: 0.7414 - acc: 0.6506\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 8s 83us/sample - loss: 0.6934 - acc: 0.6701\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 8s 84us/sample - loss: 0.6625 - acc: 0.6827\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 8s 88us/sample - loss: 0.6369 - acc: 0.6926\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 8s 86us/sample - loss: 0.6178 - acc: 0.6990\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 8s 85us/sample - loss: 0.6019 - acc: 0.7060\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 8s 86us/sample - loss: 0.5884 - acc: 0.7077\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 8s 89us/sample - loss: 0.5765 - acc: 0.7119\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.5653 - acc: 0.7155\n",
      "evaluate model: L128-D0.33-H2\n",
      "5000/5000 [==============================] - 0s 83us/sample - loss: 0.7938 - acc: 0.6386\n",
      "training model: L128-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 88us/sample - loss: 0.8844 - acc: 0.5695\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.7569 - acc: 0.6442\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 8s 88us/sample - loss: 0.7070 - acc: 0.6666\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.6733 - acc: 0.6794\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 8s 82us/sample - loss: 0.6507 - acc: 0.6867\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.6312 - acc: 0.6959\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.6154 - acc: 0.7003\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.6013 - acc: 0.7037\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.5898 - acc: 0.7088\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.5786 - acc: 0.7114\n",
      "evaluate model: L128-D0.5-H2\n",
      "5000/5000 [==============================] - 0s 86us/sample - loss: 0.8114 - acc: 0.6356\n"
     ]
    }
   ],
   "source": [
    "# build and train model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "LAYER_SIZE = [32, 64, 128]\n",
    "DROPOUT = [0, 0.33, 0.5]\n",
    "HIDDEN = [1, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for size in LAYER_SIZE:\n",
    "    for drop in DROPOUT:\n",
    "        for hidden in HIDDEN:\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.Dense(ONE_HOT_SIZE))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        for h in range(0, hidden)\n",
    "            model.add(tf.keras.layers.Dense(size))\n",
    "            model.add(Activation(\"relu\"))\n",
    "            model.add(Dropout(drop))\n",
    "\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(len(LANGUAGES))) # output layer\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer = 'adam',\n",
    "                     metrics = ['accuracy'])\n",
    "        print(\"training model: L{}-D{}-H{}\".format(size, drop, hidden))\n",
    "        model.fit(X_train, y_train, batch_size = 64, validation_split = 0.0, epochs = 10, verbose = 1)\n",
    "        NAME = \"English-German-French-L{}-D{}-H{}.model\".format(size, drop, hidden)\n",
    "        model.save(NAME)\n",
    "        print(\"evaluate model: L{}-D{}-H{}\".format(size, drop, hidden))\n",
    "        model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 56us/sample - loss: 0.7823 - acc: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7823201487541199, 0.6404]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: English\n",
      "Confidence: 99.62353110313416%\n"
     ]
    }
   ],
   "source": [
    "# change the word to test\n",
    "word = \"wiggly\"\n",
    "\n",
    "word_vec = np.array(wordToInput(word)).reshape(-1, ONE_HOT_SIZE)\n",
    "pred = model.predict(word_vec)\n",
    "\n",
    "print(\"Guess: {}\".format(LANGUAGES[np.argmax(pred)]))\n",
    "\n",
    "print(\"Confidence: {}%\".format(pred[0][np.argmax(pred)]*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
