{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\" # list of character\n",
    "MAX_WORD_LEN = 15 # longest word our network can parse\n",
    "ONE_HOT_SIZE = len(ALPHABET) * MAX_WORD_LEN # how long our one hot array should be\n",
    "\n",
    "# get position of a char in alphabet\n",
    "def getPos(char):\n",
    "    if len(char) > 1:\n",
    "        raise Exception(\"char should be a single character, length was: {}\".format(len(char)))\n",
    "    char = char.lower()\n",
    "    if char not in ALPHABET:\n",
    "        raise Exception(\"char should be in the alphabet, char was: {}\".format(char))\n",
    "    return ALPHABET.index(char)\n",
    "\n",
    "# create a one hot array for value, length of length\n",
    "def oneHot(val, length):\n",
    "    arr = [0]*length\n",
    "    arr[val] = 1\n",
    "    return arr\n",
    "\n",
    "# get one hot array for single char\n",
    "def charOneHot(char):\n",
    "    val = getPos(char)\n",
    "    return oneHot(val, len(ALPHABET))\n",
    "\n",
    "# get one hot arrays for each char in a string\n",
    "def oneHotString(string):\n",
    "    strLen = len(string)\n",
    "    result = [0]*strLen\n",
    "    for i in range(0, strLen):\n",
    "        result[i] = charOneHot(string[i])\n",
    "        \n",
    "    return result\n",
    "\n",
    "# transform a word to a one hot array \n",
    "def wordToInput(word):\n",
    "    if len(word) > MAX_WORD_LEN:\n",
    "        raise Exception(\"Word is too long, must be < 15 char, actual: {}\".format(len(word)))\n",
    "    result = [0]*ONE_HOT_SIZE\n",
    "    index = 0\n",
    "    for char in word:\n",
    "        result[getPos(char) + index] = 1\n",
    "        index = index + len(ALPHABET)\n",
    "    return result\n",
    "    \n",
    "# great, now we can turn words into one hot arrays\n",
    "# now we have to get and label training data\n",
    "\n",
    "# functions for data wrangling\n",
    "\n",
    "# returns true if word only contains letters in alphabet\n",
    "def isAlpha(word):\n",
    "    word = word.lower()\n",
    "    for char in word:\n",
    "        if char not in ALPHABET:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts before slice:\n",
      "English words: 44295\n",
      "German words: 39692\n",
      "French words: 33678 \n",
      "\n",
      "Word counts after slice:\n",
      "English words: 33000\n",
      "German words: 33000\n",
      "French words: 33000\n"
     ]
    }
   ],
   "source": [
    "LANGUAGES = [\"English\", \"German\", \"French\"]\n",
    "MIN_WORD_LEN = 3\n",
    "\n",
    "# dictionary source: https://github.com/hermitdave/FrequencyWords\n",
    "\n",
    "# data wrangling time, things we have to do:\n",
    "# remove words shorter than 3(not required but it'll help) \n",
    "# remove words containing words outside of the english alphabet (this means no accents)\n",
    "# remove words longer than 15 characters (or whatever we decide our maximum to be)\n",
    "# the dataset I'm using also has frequency for each word, we don't need that\n",
    "\n",
    "# parsing dictionaries\n",
    "english = open(\"data/english_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "english_lines = english.readlines()\n",
    "english_words = []\n",
    "for line in english_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        english_words.append(word)        \n",
    "        \n",
    "german = open(\"data/german_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "german_lines = german.readlines()\n",
    "german_words = []\n",
    "for line in german_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        german_words.append(word)\n",
    "        \n",
    "french = open(\"data/french_dict.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "french_lines = french.readlines()\n",
    "french_words = []\n",
    "for line in french_lines:\n",
    "    split = line.split(\" \")\n",
    "    word = split[0]\n",
    "    if len(word) > MIN_WORD_LEN and len(word) < MAX_WORD_LEN and isAlpha(word):\n",
    "        french_words.append(word)\n",
    "        \n",
    "print(\"Word counts before slice:\")\n",
    "print(\"English words: {}\".format(len(english_words)))\n",
    "print(\"German words: {}\".format(len(german_words)))\n",
    "print(\"French words: {} \\n\".format(len(french_words)))\n",
    "\n",
    "\n",
    "#balancing\n",
    "SAMPLES = 33000 # number of words for each language, should be less than smallest dictionary\n",
    "\n",
    "english_words = english_words[0:SAMPLES]\n",
    "german_words = german_words[0:SAMPLES]\n",
    "french_words = french_words[0:SAMPLES]\n",
    "\n",
    "print(\"Word counts after slice:\")\n",
    "print(\"English words: {}\".format(len(english_words)))\n",
    "print(\"German words: {}\".format(len(german_words)))\n",
    "print(\"French words: {}\".format(len(french_words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to label data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# converting words to one hot encoding\n",
    "X_train = []\n",
    "for word in english_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "\n",
    "for word in german_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "    \n",
    "for word in french_words:\n",
    "    X_train.append(wordToInput(word))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# labeling data\n",
    "y_train = []\n",
    "y_train.extend([0]*SAMPLES) #English\n",
    "y_train.extend([1]*SAMPLES) #German\n",
    "y_train.extend([2]*SAMPLES) #French\n",
    "\n",
    "# converting labels to one hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# shuffling data\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "# data labeled in np array\n",
    "\n",
    "\n",
    "# Validation slice\n",
    "\n",
    "NUM_VALIDATE = 5000\n",
    "\n",
    "X_test = X_train[len(X_train) - NUM_VALIDATE:len(X_train)]\n",
    "y_test = y_train[len(y_train) - NUM_VALIDATE:len(y_train)]\n",
    "\n",
    "X_train = X_train[0:len(X_train) - NUM_VALIDATE]\n",
    "y_train = y_train[0:len(y_train) - NUM_VALIDATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to load serialized data\n",
    "import pickle\n",
    "X_train = pickle.load(open(\"X.pickle\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model: L32-D0-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 6s 64us/sample - loss: 0.8512 - acc: 0.5915\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 62us/sample - loss: 0.7336 - acc: 0.6557\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 6s 68us/sample - loss: 0.6860 - acc: 0.6757\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 6s 61us/sample - loss: 0.6550 - acc: 0.6883\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 6s 62us/sample - loss: 0.6319 - acc: 0.6977\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 6s 63us/sample - loss: 0.6120 - acc: 0.7044\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 6s 60us/sample - loss: 0.5960 - acc: 0.7092\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.5826 - acc: 0.7143\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 6s 64us/sample - loss: 0.5704 - acc: 0.7174\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 5s 58us/sample - loss: 0.5604 - acc: 0.7193\n",
      "evaluate model: L32-D0-H1\n",
      "5000/5000 [==============================] - 0s 81us/sample - loss: 0.8207 - acc: 0.6324\n",
      "training model: L32-D0-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.8459 - acc: 0.5945\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 6s 67us/sample - loss: 0.7276 - acc: 0.6571\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 6s 66us/sample - loss: 0.6835 - acc: 0.6769\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 6s 65us/sample - loss: 0.6539 - acc: 0.6877\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 6s 63us/sample - loss: 0.6307 - acc: 0.6972\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.6100 - acc: 0.7033\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 6s 66us/sample - loss: 0.5957 - acc: 0.7081\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 6s 65us/sample - loss: 0.5808 - acc: 0.7131\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 6s 66us/sample - loss: 0.5690 - acc: 0.7163\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 6s 65us/sample - loss: 0.5586 - acc: 0.7189\n",
      "evaluate model: L32-D0-H2\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.7992 - acc: 0.6306\n",
      "training model: L32-D0.33-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 83us/sample - loss: 0.8701 - acc: 0.5828\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.7519 - acc: 0.6464\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.7035 - acc: 0.6691\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6720 - acc: 0.6801\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6481 - acc: 0.6908\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6296 - acc: 0.6958\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.6121 - acc: 0.7011\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 78us/sample - loss: 0.5988 - acc: 0.7047\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5877 - acc: 0.7094\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.5770 - acc: 0.7110\n",
      "evaluate model: L32-D0.33-H1\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.8012 - acc: 0.6468\n",
      "training model: L32-D0.33-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.9050 - acc: 0.5599\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.7737 - acc: 0.6378\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.7215 - acc: 0.6603\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6876 - acc: 0.6752\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6633 - acc: 0.6821\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.6456 - acc: 0.6892\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6284 - acc: 0.6933\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.6153 - acc: 0.6982\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.6032 - acc: 0.7022\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 80us/sample - loss: 0.5912 - acc: 0.7041\n",
      "evaluate model: L32-D0.33-H2\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.8007 - acc: 0.6464\n",
      "training model: L32-D0.5-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 82us/sample - loss: 0.8925 - acc: 0.5691\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.7737 - acc: 0.6384\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.7229 - acc: 0.6610\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6912 - acc: 0.6745\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6681 - acc: 0.6841\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6482 - acc: 0.6902\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6318 - acc: 0.6940\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 78us/sample - loss: 0.6169 - acc: 0.6985\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.6044 - acc: 0.7012\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.5946 - acc: 0.7041\n",
      "evaluate model: L32-D0.5-H1\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.7932 - acc: 0.6450\n",
      "training model: L32-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.9406 - acc: 0.5315\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 80us/sample - loss: 0.8084 - acc: 0.6233\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 8s 89us/sample - loss: 0.7542 - acc: 0.6490\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 9s 94us/sample - loss: 0.7202 - acc: 0.6642\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.6959 - acc: 0.6711\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 8s 84us/sample - loss: 0.6756 - acc: 0.6774\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 8s 87us/sample - loss: 0.6570 - acc: 0.6851\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6464 - acc: 0.6885\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 70us/sample - loss: 0.6340 - acc: 0.6923\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.6252 - acc: 0.6953\n",
      "evaluate model: L32-D0.5-H2\n",
      "5000/5000 [==============================] - 0s 99us/sample - loss: 0.7892 - acc: 0.6470\n",
      "training model: L64-D0-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 82us/sample - loss: 0.8441 - acc: 0.5964\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.7268 - acc: 0.6579\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.6801 - acc: 0.6783\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.6501 - acc: 0.6889\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6265 - acc: 0.6978\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.6072 - acc: 0.7060\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 73us/sample - loss: 0.5915 - acc: 0.7100\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 70us/sample - loss: 0.5779 - acc: 0.7148\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.5663 - acc: 0.7180\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 70us/sample - loss: 0.5546 - acc: 0.7212\n",
      "evaluate model: L64-D0-H1\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.8099 - acc: 0.6344\n",
      "training model: L64-D0-H2\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94000/94000 [==============================] - 7s 77us/sample - loss: 0.8425 - acc: 0.5941\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.7278 - acc: 0.6574\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.6814 - acc: 0.6774\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.6496 - acc: 0.6895\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.6259 - acc: 0.6977\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.6048 - acc: 0.7049\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.5885 - acc: 0.7096\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 72us/sample - loss: 0.5741 - acc: 0.7150\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5634 - acc: 0.7177\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.5513 - acc: 0.7214\n",
      "evaluate model: L64-D0-H2\n",
      "5000/5000 [==============================] - 0s 99us/sample - loss: 0.8291 - acc: 0.6330\n",
      "training model: L64-D0.33-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 7s 79us/sample - loss: 0.8630 - acc: 0.5839\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.7422 - acc: 0.6501\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6959 - acc: 0.6714\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6622 - acc: 0.6849\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.6379 - acc: 0.6940\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 7s 75us/sample - loss: 0.6196 - acc: 0.7001\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 7s 74us/sample - loss: 0.6025 - acc: 0.7059\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.5892 - acc: 0.7088\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 7s 71us/sample - loss: 0.5773 - acc: 0.7123\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 7s 76us/sample - loss: 0.5666 - acc: 0.7160\n",
      "evaluate model: L64-D0.33-H1\n",
      "5000/5000 [==============================] - 1s 109us/sample - loss: 0.8005 - acc: 0.6420\n",
      "training model: L64-D0.33-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 8s 86us/sample - loss: 0.8779 - acc: 0.5742\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 8s 82us/sample - loss: 0.7556 - acc: 0.6444\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 9s 92us/sample - loss: 0.7058 - acc: 0.6669\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 10s 105us/sample - loss: 0.6721 - acc: 0.6804\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 9s 99us/sample - loss: 0.6488 - acc: 0.6878\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 10s 103us/sample - loss: 0.6283 - acc: 0.6960\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 11s 116us/sample - loss: 0.6119 - acc: 0.7004\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.5997 - acc: 0.7052\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 11s 115us/sample - loss: 0.5877 - acc: 0.7087\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 10s 110us/sample - loss: 0.5765 - acc: 0.7114\n",
      "evaluate model: L64-D0.33-H2\n",
      "5000/5000 [==============================] - 1s 145us/sample - loss: 0.7982 - acc: 0.6314\n",
      "training model: L64-D0.5-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 11s 113us/sample - loss: 0.8761 - acc: 0.5772\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.7562 - acc: 0.6438\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.7066 - acc: 0.6671\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 10s 107us/sample - loss: 0.6749 - acc: 0.6798\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 10s 111us/sample - loss: 0.6510 - acc: 0.6879\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 10s 106us/sample - loss: 0.6326 - acc: 0.6942\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 10s 107us/sample - loss: 0.6147 - acc: 0.7002\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.6007 - acc: 0.7026\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 9s 98us/sample - loss: 0.5901 - acc: 0.7068\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 86us/sample - loss: 0.5786 - acc: 0.7109\n",
      "evaluate model: L64-D0.5-H1\n",
      "5000/5000 [==============================] - 1s 116us/sample - loss: 0.7941 - acc: 0.6408\n",
      "training model: L64-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 9s 100us/sample - loss: 0.9077 - acc: 0.5557\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 10s 110us/sample - loss: 0.7777 - acc: 0.6360\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 11s 112us/sample - loss: 0.7235 - acc: 0.6603\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 10s 102us/sample - loss: 0.6893 - acc: 0.6730\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 9s 101us/sample - loss: 0.6643 - acc: 0.6824\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 9s 98us/sample - loss: 0.6463 - acc: 0.6892\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 10s 104us/sample - loss: 0.6304 - acc: 0.6954\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 10s 102us/sample - loss: 0.6175 - acc: 0.7000\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 10s 101us/sample - loss: 0.6045 - acc: 0.7031\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 9s 99us/sample - loss: 0.5951 - acc: 0.7054\n",
      "evaluate model: L64-D0.5-H2\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.8085 - acc: 0.6362\n",
      "training model: L128-D0-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 10s 109us/sample - loss: 0.8427 - acc: 0.5973\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 9s 100us/sample - loss: 0.7258 - acc: 0.6573\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 10s 102us/sample - loss: 0.6790 - acc: 0.6775\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 9s 92us/sample - loss: 0.6472 - acc: 0.6912\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 9s 95us/sample - loss: 0.6233 - acc: 0.6984\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 9s 100us/sample - loss: 0.6029 - acc: 0.7062\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 9s 92us/sample - loss: 0.5863 - acc: 0.7101\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.5725 - acc: 0.7150\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.5594 - acc: 0.7181\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 8s 81us/sample - loss: 0.5474 - acc: 0.7236\n",
      "evaluate model: L128-D0-H1\n",
      "5000/5000 [==============================] - 1s 114us/sample - loss: 0.8399 - acc: 0.6296\n",
      "training model: L128-D0-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 10s 103us/sample - loss: 0.8380 - acc: 0.5976\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 9s 92us/sample - loss: 0.7231 - acc: 0.6579\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 9s 97us/sample - loss: 0.6756 - acc: 0.6789\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 9s 98us/sample - loss: 0.6441 - acc: 0.6901\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 9s 94us/sample - loss: 0.6194 - acc: 0.6989\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 9s 94us/sample - loss: 0.5979 - acc: 0.7051\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 9s 93us/sample - loss: 0.5823 - acc: 0.7114\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 9s 95us/sample - loss: 0.5655 - acc: 0.7148\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 9s 100us/sample - loss: 0.5522 - acc: 0.7208\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 9s 94us/sample - loss: 0.5405 - acc: 0.7227\n",
      "evaluate model: L128-D0-H2\n",
      "5000/5000 [==============================] - 1s 120us/sample - loss: 0.8745 - acc: 0.6342\n",
      "training model: L128-D0.33-H1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.8548 - acc: 0.5881\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 10s 109us/sample - loss: 0.7369 - acc: 0.6536\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 11s 112us/sample - loss: 0.6868 - acc: 0.6742\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 10s 108us/sample - loss: 0.6545 - acc: 0.6873\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 10s 112us/sample - loss: 0.6297 - acc: 0.6961\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 10s 107us/sample - loss: 0.6095 - acc: 0.7022\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 10s 107us/sample - loss: 0.5932 - acc: 0.7078\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 10s 110us/sample - loss: 0.5797 - acc: 0.7121\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 10s 110us/sample - loss: 0.5692 - acc: 0.7149\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 10s 106us/sample - loss: 0.5573 - acc: 0.7196\n",
      "evaluate model: L128-D0.33-H1\n",
      "5000/5000 [==============================] - 1s 143us/sample - loss: 0.8015 - acc: 0.6352\n",
      "training model: L128-D0.33-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 12s 132us/sample - loss: 0.8635 - acc: 0.5812\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 12s 125us/sample - loss: 0.7433 - acc: 0.6499\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 12s 127us/sample - loss: 0.6940 - acc: 0.6708\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 12s 129us/sample - loss: 0.6636 - acc: 0.6832\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 12s 130us/sample - loss: 0.6396 - acc: 0.6913\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 12s 122us/sample - loss: 0.6195 - acc: 0.6982\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 12s 125us/sample - loss: 0.6038 - acc: 0.7043\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 12s 123us/sample - loss: 0.5909 - acc: 0.7080\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 12s 124us/sample - loss: 0.5776 - acc: 0.7105\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 12s 124us/sample - loss: 0.5681 - acc: 0.7146\n",
      "evaluate model: L128-D0.33-H2\n",
      "5000/5000 [==============================] - 1s 152us/sample - loss: 0.8128 - acc: 0.6420\n",
      "training model: L128-D0.5-H1\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 11s 118us/sample - loss: 0.8627 - acc: 0.5856\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 10s 111us/sample - loss: 0.7420 - acc: 0.6504\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 10s 112us/sample - loss: 0.6932 - acc: 0.6732\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 10s 111us/sample - loss: 0.6612 - acc: 0.6842\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 11s 112us/sample - loss: 0.6381 - acc: 0.6934\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 11s 112us/sample - loss: 0.6185 - acc: 0.6993\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 10s 103us/sample - loss: 0.6031 - acc: 0.7054\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 9s 97us/sample - loss: 0.5900 - acc: 0.7087\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 9s 99us/sample - loss: 0.5770 - acc: 0.7124\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 9s 100us/sample - loss: 0.5664 - acc: 0.7144\n",
      "evaluate model: L128-D0.5-H1\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.8024 - acc: 0.6450\n",
      "training model: L128-D0.5-H2\n",
      "Epoch 1/10\n",
      "94000/94000 [==============================] - 12s 127us/sample - loss: 0.8865 - acc: 0.5670\n",
      "Epoch 2/10\n",
      "94000/94000 [==============================] - 11s 114us/sample - loss: 0.7582 - acc: 0.6441\n",
      "Epoch 3/10\n",
      "94000/94000 [==============================] - 11s 114us/sample - loss: 0.7080 - acc: 0.6660\n",
      "Epoch 4/10\n",
      "94000/94000 [==============================] - 11s 114us/sample - loss: 0.6757 - acc: 0.6778\n",
      "Epoch 5/10\n",
      "94000/94000 [==============================] - 11s 113us/sample - loss: 0.6525 - acc: 0.6863\n",
      "Epoch 6/10\n",
      "94000/94000 [==============================] - 11s 112us/sample - loss: 0.6333 - acc: 0.6935\n",
      "Epoch 7/10\n",
      "94000/94000 [==============================] - 11s 114us/sample - loss: 0.6165 - acc: 0.6993\n",
      "Epoch 8/10\n",
      "94000/94000 [==============================] - 10s 111us/sample - loss: 0.6039 - acc: 0.7035\n",
      "Epoch 9/10\n",
      "94000/94000 [==============================] - 10s 109us/sample - loss: 0.5910 - acc: 0.7083\n",
      "Epoch 10/10\n",
      "94000/94000 [==============================] - 10s 110us/sample - loss: 0.5801 - acc: 0.7110\n",
      "evaluate model: L128-D0.5-H2\n",
      "5000/5000 [==============================] - 1s 140us/sample - loss: 0.8090 - acc: 0.6420\n"
     ]
    }
   ],
   "source": [
    "# build and train model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "LAYER_SIZE = [32, 64, 128]\n",
    "DROPOUT = [0, 0.33, 0.5]\n",
    "HIDDEN = [1, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for size in LAYER_SIZE:\n",
    "    for drop in DROPOUT:\n",
    "        for hidden in HIDDEN:\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(tf.keras.layers.Dense(ONE_HOT_SIZE))\n",
    "            model.add(Activation(\"relu\"))\n",
    "            for h in range(0, hidden):\n",
    "                model.add(tf.keras.layers.Dense(size))\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(Dropout(drop))\n",
    "\n",
    "\n",
    "            model.add(tf.keras.layers.Dense(len(LANGUAGES))) # output layer\n",
    "            model.add(Activation(\"softmax\"))\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                         optimizer = 'adam',\n",
    "                         metrics = ['accuracy'])\n",
    "            print(\"training model: L{}-D{}-H{}\".format(size, drop, hidden))\n",
    "            model.fit(X_train, y_train, batch_size = 64, validation_split = 0.0, epochs = 10, verbose = 1)\n",
    "            NAME = \"models/English-German-French-L{}-D{}-H{}.model\".format(size, drop, hidden)\n",
    "            model.save(NAME)\n",
    "            print(\"evaluate model: L{}-D{}-H{}\".format(size, drop, hidden))\n",
    "            model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 56us/sample - loss: 0.7823 - acc: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7823201487541199, 0.6404]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: English\n",
      "Confidence: 99.62353110313416%\n"
     ]
    }
   ],
   "source": [
    "# change the word to test\n",
    "word = \"wiggly\"\n",
    "\n",
    "word_vec = np.array(wordToInput(word)).reshape(-1, ONE_HOT_SIZE)\n",
    "pred = model.predict(word_vec)\n",
    "\n",
    "print(\"Guess: {}\".format(LANGUAGES[np.argmax(pred)]))\n",
    "\n",
    "print(\"Confidence: {}%\".format(pred[0][np.argmax(pred)]*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
